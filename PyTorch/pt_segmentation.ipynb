{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/oem/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "Using cache found in /home/oem/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "# # Converting a Segmentation Model via CoreML -- https://developer.apple.com/videos/play/tech-talks/10154\n",
    "\n",
    "# ### Imports\n",
    "\n",
    "import urllib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import json\n",
    "\n",
    "from torchvision import transforms\n",
    "import coremltools as ct\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# ### Load Sample Model and Image\n",
    "\n",
    "# Load model\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True).eval()\n",
    "# Load sample image\n",
    "input_image = Image.open(\"dog_and_cat.jpg\")\n",
    "input_image = input_image.resize((224, 224))\n",
    "\n",
    "# ### Image Preprocessing\n",
    "\n",
    "to_tensor = transforms.ToTensor()\n",
    "input_tensor = to_tensor(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "\n",
    "# ### Trace the Model with PyTorch\n",
    "\n",
    "# ### Wrap the Model to Allow Tracing\n",
    "\n",
    "class WrappedDeeplabv3Resnet101(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(WrappedDeeplabv3Resnet101, self).__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True).eval()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        res = self.model(x)\n",
    "        x = res[\"out\"]\n",
    "        return x\n",
    "\n",
    "\n",
    "# ### Trace the Wrapped Model\n",
    "\n",
    "traceable_model = WrappedDeeplabv3Resnet101().eval()\n",
    "trace = torch.jit.trace(traceable_model, input_batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 844/845 [00:00<00:00, 5522.39 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 268.93 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 56/56 [00:00<00:00, 60.48 passes/s] \n",
      "Running MIL backend_neuralnetwork pipeline: 100%|██████████| 8/8 [00:00<00:00, 455.68 passes/s]\n",
      "Translating MIL ==> NeuralNetwork Ops: 100%|██████████| 944/944 [00:02<00:00, 341.83 ops/s] \n"
     ]
    }
   ],
   "source": [
    "# ### Convert to Core ML \n",
    "\n",
    "# Define input\n",
    "_input = ct.ImageType(\n",
    "    name=\"input_1\", \n",
    "    shape=input_batch.shape, \n",
    "    bias=[-0.485/0.229,-0.456/0.224,-0.406/0.225], \n",
    "    scale= 1./(255*0.226)\n",
    ")\n",
    "\n",
    "# Convert model\n",
    "mlmodel = ct.convert(\n",
    "    trace,\n",
    "    inputs=[_input],\n",
    ")\n",
    "\n",
    "\n",
    "# ### Set the Model Metadata\n",
    "\n",
    "labels_json = {\"labels\": [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"board\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningTable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedPlant\", \"sheep\", \"sofa\", \"train\", \"tvOrMonitor\"]}\n",
    "\n",
    "mlmodel.type = 'imageSegmenter'\n",
    "mlmodel.user_defined_metadata['com.apple.coreml.model.preview.params'] = json.dumps(labels_json)\n",
    "\n",
    "\n",
    "# ### Save the Model for Visualization\n",
    "\n",
    "mlmodel.save(\"SegmentationModel.mlmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Mobile no metal support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0 INTERNAL ASSERT FAILED at \"../torch/csrc/jit/ir/alias_analysis.cpp\":608, please report a bug to PyTorch. We don't have an op for metal_prepack::conv2d_prepack but it isn't a special case.  Argument types: Tensor, NoneType, int[], int[], int[], int, NoneType, NoneType, \n\nCandidates:",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimized_model \u001b[39m=\u001b[39m optimize_for_mobile(trace, backend\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mMetal\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m optimized_model\u001b[39m.\u001b[39m_save_for_lite_interpreter(\u001b[39m\"\u001b[39m\u001b[39mdeeplabv3_resnet101_scripted.ptl\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tsz/lib/python3.9/site-packages/torch/utils/mobile_optimizer.py:69\u001b[0m, in \u001b[0;36moptimize_for_mobile\u001b[0;34m(script_module, optimization_blocklist, preserved_methods, backend)\u001b[0m\n\u001b[1;32m     67\u001b[0m     optimized_cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_jit_pass_vulkan_optimize_for_mobile(script_module\u001b[39m.\u001b[39m_c, preserved_methods_str)\n\u001b[1;32m     68\u001b[0m \u001b[39melif\u001b[39;00m backend \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmetal\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 69\u001b[0m     optimized_cpp_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_jit_pass_metal_optimize_for_mobile(script_module\u001b[39m.\u001b[39;49m_c, preserved_methods_str)\n\u001b[1;32m     70\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown backend, must be one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mCPU\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mVulkan\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mMetal\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0 INTERNAL ASSERT FAILED at \"../torch/csrc/jit/ir/alias_analysis.cpp\":608, please report a bug to PyTorch. We don't have an op for metal_prepack::conv2d_prepack but it isn't a special case.  Argument types: Tensor, NoneType, int[], int[], int[], int, NoneType, NoneType, \n\nCandidates:"
     ]
    }
   ],
   "source": [
    "optimized_model = optimize_for_mobile(trace, backend='Metal')\n",
    "optimized_model._save_for_lite_interpreter(\"deeplabv3_resnet101_scripted.ptl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CPU only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = optimize_for_mobile(trace)\n",
    "optimized_model._save_for_lite_interpreter(\"deeplabv3_resnet101_scripted.ptl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
